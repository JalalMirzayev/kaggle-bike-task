{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FYLAxCmklHVq"
   },
   "source": [
    "# Hands On Data Analytics\n",
    "\n",
    "This data set is based on the Kaggle challenge [Bike Sharing Demand](https://www.kaggle.com/competitions/bike-sharing-demand/overview).\n",
    "\n",
    "Bike sharing systems are a means of renting bicycles where the process of obtaining membership, rental, and bike return is automated via a network of kiosk locations throughout a city.\n",
    "\n",
    "- Capital Bikeshare program in Washington D.C. collects detailed data\n",
    "- The data set that will be used in the project is often used for research. A similar task was provided in a Kaggle challege.\n",
    "- The goal of this project is to predict the bike sharing demand (variable `count`) in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "go-XhaK_wLwn"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries and set the style of the plots\n",
    "import asyncio\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from IPython.core.display import HTML\n",
    "\n",
    "# Logic to keep binder notebook alive\n",
    "async def keep_me_alive():\n",
    "    while True:\n",
    "        await asyncio.sleep(120)\n",
    "        x = 1\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.create_task(keep_me_alive())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-jwsJ-GCwiKy"
   },
   "source": [
    "# 1. Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X316auS1xSuG"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"https://raw.githubusercontent.com/JalalMirzayev/kaggle-bike-task/main/data/data.csv\", index_col=\"datetime\", date_parser=pd.to_datetime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "ljm_-xy7Ltmn",
    "outputId": "66d7589c-3725-4196-b27d-b865bf26a6c6"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pjIQq2sdlbVT"
   },
   "source": [
    "You are provided hourly rental data spanning two years.\n",
    "\n",
    "Data Fields\n",
    "- datetime: hourly date + timestamp  \n",
    "- season: 1 = spring, 2 = summer, 3 = fall, 4 = winter \n",
    "- holiday: whether the day is considered a holiday\n",
    "- workingday: whether the day is neither a weekend nor holiday\n",
    "- weather: \n",
    "  - 1: Clear, Few clouds, Partly cloudy, Partly cloudy\n",
    "  - 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n",
    "  - 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds\n",
    "  - 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \n",
    "- temp: temperature in Celsius\n",
    "- atemp: \"feels like\" temperature in Celsius\n",
    "- humidity: relative humidity\n",
    "- windspeed: wind speed\n",
    "- count: number of total rentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VG4fkoSVMKYH"
   },
   "source": [
    "# 2. Data exploration & Feature engineering\n",
    "\n",
    "Print basic statistics for quantitative and categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "ryRt-DRGMZd3",
    "outputId": "f22b02ce-29e1-4f01-b1f2-399e5ab5d966"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h0J4anj5ND9K"
   },
   "source": [
    "The method `value_counts()` is a helpful for looking into categorical attributes, showing how often each unique value occurs. The code to check for unique weather categories is already prepared for you. \n",
    "\n",
    "Run the following code cell and think about the meaning of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mIMux025Msu3",
    "outputId": "770a1e03-8266-44ce-d0dc-b49303fcab2b"
   },
   "outputs": [],
   "source": [
    "df[\"weather\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uV0m3hGxNc2n"
   },
   "source": [
    "üìù **Task**: Determine `value_counts()` for two additional categorical attributes/columns/variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-3Kf_e3lM6D8",
    "outputId": "59d7eff1-dbf6-4401-b191-3c4e31706dad"
   },
   "outputs": [],
   "source": [
    "# Code for first categorical attribute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YPJZ4mZLNzH9",
    "outputId": "b29b0d30-895c-4248-ca6b-855989c1f735"
   },
   "outputs": [],
   "source": [
    "# Code for second categorical attribute\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uy3z1XsUOhe0"
   },
   "source": [
    "üìù **Task:** Try to use `value_counts()` on a continuous column like `windspeed`. What do you notice? Does it make sense to determine value_counts() for continuous attributes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Zgj7y9XN4jT",
    "outputId": "651f50b3-da39-440e-9e06-a4aa0884c185"
   },
   "outputs": [],
   "source": [
    "# Code for continuous attribute (e.g. windspeed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EGGXq8KDN9ir"
   },
   "source": [
    "## Creating insightful plots\n",
    "Charts and diagrams are a very popular way to explore data and to understand relationships. There are multiple visualization packages for Python (e.g. Matplotlib, Seaborn, Plotly, etc.). These packages can be used to create different types of visualizations (e.g. scatter, bar, line, histogram, boxplot, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "id": "sLNni8tJQV_8",
    "outputId": "5a2e7413-14b4-4640-e80f-9a742936758e"
   },
   "outputs": [],
   "source": [
    "# Plotting the output variable (count) over time gives us a first impression over the distribution\n",
    "# Plotting is usually done with matplotlib (imported as plt). We are defining the scatter plot in few steps:\n",
    "plt.figure(figsize=(20, 8))\n",
    "plt.scatter(df.index, df[\"count\"], c=df[\"temp\"], cmap=\"coolwarm\", s=5)\n",
    "plt.colorbar()\n",
    "plt.xlabel(\"Datetime\")\n",
    "plt.ylabel(\"Number of bike rentals per hour\")\n",
    "plt.title(\"Bike rentals over the course of two years\\n colorcoded by temperature\")\n",
    "display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6OS2AxSQdCL"
   },
   "source": [
    "## Splitting timestamp into features (Feature engineering)\n",
    "\n",
    "Feature engineering plays a crucial role for the performance of a machine learning algorithm. \n",
    "\n",
    "It encompasses various techniques such as \n",
    "- handling/removing outliers or missing values\n",
    "- transformation of variables \n",
    "- scaling of variables\n",
    "- combination of variables or \n",
    "- splitting a feature into several ones (e.g. date -> year, month, day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "3rGaHGMhSRF_",
    "outputId": "a7024adf-c1ae-4530-842e-6572bb2ab5ff"
   },
   "outputs": [],
   "source": [
    "# The datetime timestamp provides valuable information, we need to extract the individual attributes in order to make better use of it\n",
    "df[\"hour\"] = [datetime.hour for datetime in df.index]\n",
    "# üìù Task: Use the same procedure for day, month, and year\n",
    "df[\"day\"] = \n",
    "df[\"month\"] = \n",
    "df[\"year\"] = \n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GYJJnanSTEpj"
   },
   "source": [
    "We splitted the date into components and there is already other information extracted out of a date (season, holiday, workingday) \n",
    "\n",
    "üìù **Task**: Can you think of another potentially valuable attribute that we could try to extract?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237
    },
    "id": "pWrHQk3oS7qi",
    "outputId": "c8c118ea-49ab-4630-93a4-26d63782740b"
   },
   "outputs": [],
   "source": [
    "df[\"REPLACE_ATTRBUTE\"] = \n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hRx4obTQToEG"
   },
   "source": [
    "## Train - Test split for model training and evaluation\n",
    "\n",
    "In supervised learning, we always have a labeled training set (=the correct output) at hand. The evaluation of the model is done with previously unseen data. Therefore, we need to split our dataframe representing the full data set beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7VQ322hT34i"
   },
   "outputs": [],
   "source": [
    "# There are mutltiple ways to create a train - test split. Using the Pandas-internal sample() function is straightforward\n",
    "df_train = df.sample(frac=0.75, random_state=1)\n",
    "df_test = df.drop(df_train.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UDmq12gmUUPa"
   },
   "outputs": [],
   "source": [
    "# We need to break down the train - test sets into independent variables (input features: what we have as input) and dependent variable (target: what we want to predict)\n",
    "x_train = df_train.drop([\"count\"], axis=1)\n",
    "y_train = df_train[\"count\"]\n",
    "\n",
    "x_test = df_test.drop([\"count\"], axis=1)\n",
    "y_test = df_test[\"count\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpFS-83oUoq6"
   },
   "source": [
    "# 3. Supervised Machine Learning with Decision Trees\n",
    "\n",
    "### Training a decision tree model\n",
    "Decision Trees based algorithms are very popular in machine learning. They are easy to set-up, capable of capturing non-linearity, good at handling categorical variables and intuitive to understand. Trees are built from the top to the ground, branches and their associated splitting criteria are always created in order to improve a given metric. Being greedy algorithms, they always search for the local optimum at every split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zj420umMVBqL",
    "outputId": "741f4163-e39a-4a32-cdce-70c478c0de4e"
   },
   "outputs": [],
   "source": [
    "# The object of the model with selected parameters has to be created. max_depth specifies how many layers we want in the tree, criterion defines the splitting metric\n",
    "model1 = DecisionTreeRegressor(criterion='absolute_error', max_depth=3)\n",
    "\n",
    "# Now the model becomes trained with fit() using the previously created training data\n",
    "model1.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJgMHygDVN5I"
   },
   "source": [
    "### Predicting results and visualizing the decision tree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "id": "XvDCIRpqVvWR",
    "outputId": "8b3e7eec-8667-42bc-f116-f959b9216ce0"
   },
   "outputs": [],
   "source": [
    "# The trained model is now ready to predict any new input having the same attributes as the training data\n",
    "y_predictions1 = model1.predict(x_test)\n",
    "\n",
    "# The output is simply a list of the forecasted bike rentals in the test set\n",
    "print(\"y_predictions: \", y_predictions1[:5])\n",
    "\n",
    "# Print input features\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "1UxGA_qWV25q",
    "outputId": "69554adf-7b2f-478b-a281-f19f3bc56518"
   },
   "outputs": [],
   "source": [
    "HTML('<img src=\"https://raw.githubusercontent.com/JalalMirzayev/kaggle-bike-task/main/images/decision_tree_plot.png\" style=\"width:800px;height:400px;\">')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dxrczs1tXP4M",
    "outputId": "f0f287c4-07ce-4d32-e9bc-1aa7e7299267"
   },
   "outputs": [],
   "source": [
    "# Let us look at an individual case and follow the path of our decision tree\n",
    "print('Prediction: ' + str(y_predictions1[2]))\n",
    "print(df_test.iloc[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QzoQtqFCYl3D"
   },
   "source": [
    "### Evaluating model performance\n",
    "Many different evaluation metrics can be used to assess a model's performance.\n",
    "The following three metrics are often used to evaluate the performance of regression models\n",
    "\n",
    "<hr style=\"border:2px solid gray\"> </hr>\n",
    "\n",
    "**Mean Squared Error**. The Mean Squared Error (MSE) is defined as follows\n",
    "\n",
    "$$ \\text{MSE} = \\frac{1}{N}\\sum_{n=1}^N[y_\\text{n, actual} - y_\\text{n,predict}]^2,$$\n",
    "\n",
    "in which $y_\\text{n,predict}$ is the prediction for the $n^\\text{th}$ observation. The value $y_\\text{n, actual}$ is the actual value of the $n^\\text{th}$ observation.\n",
    "\n",
    "<hr style=\"border:2px solid gray\"> </hr>\n",
    "\n",
    "**Mean Absolute Error**. The Mean Absolute Error (MAE) is defined as follows\n",
    "\n",
    "$$ \\text{MSE} = \\frac{1}{N}\\sum_{n=1}^N\\left|y_\\text{n, actual} - y_\\text{n,predict}\\right|^2,$$\n",
    "\n",
    "in which $y_\\text{n,predict}$ is the prediction for the $n^\\text{th}$ observation. The value $y_\\text{n, actual}$ is the actual value of the $n^\\text{th}$ observation.\n",
    "\n",
    "<hr style=\"border:2px solid gray\"> </hr>\n",
    "\n",
    "**Coefficient of determination**. The coefficient of determination (also known as $R^2$ or $R$ squared, ger. Bestimmtheitsma√ü/Determinationskoeffizient) is defined as follows\n",
    "\n",
    "$$R^2 = \\dfrac{\\sum_{n=1}^N[y_\\text{n,predict}-\\overline{y}]^2}{\\sum_{n=1}^N[y_\\text{n,actual}-\\overline{y}]^2 },$$\n",
    "\n",
    "in which $y_\\text{n,predict}$ is the prediction for the $n^\\text{th}$ observation. The value $y_\\text{n, actual}$ is the actual value of the $n^\\text{th}$ observation. Finally, $\\overline{y}$ is the mean of all actual obervations $y_\\text{n, actual}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mCZMMuXSvT6v",
    "outputId": "cad4e24e-ca95-4c83-f06e-f30d67fc4431"
   },
   "outputs": [],
   "source": [
    "print(\"mean squared error: \", mean_squared_error(y_test, y_predictions1))\n",
    "print(\"mean absolute error: \", mean_absolute_error(y_test, y_predictions1))\n",
    "print(\"R¬≤: \", r2_score(y_test, y_predictions1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMoJPPJcvpU1"
   },
   "source": [
    "## Hyperparameter tuning\n",
    "The Decision Tree has several parameters which can be tuned. It's an important part of machine learning to set appropriate values for these parameters.\n",
    "\n",
    "**Code as reference**\n",
    "```bash\n",
    "model2 = DecisionTreeRegressor(criterion=\"absolute_error\", max_depth=3)\n",
    "model2.fit(x_train, y_train)\n",
    "y_predictions2 = model2.predict(x_test)\n",
    "```\n",
    "\n",
    "üìù **Task**: Try different values for `max_depth` (e.g. 100, 200, and 300) and compare the performance metrics. Use the code above for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eCNtx0HkzlTf"
   },
   "outputs": [],
   "source": [
    "# Enter your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TuMjwqa5Y2JW",
    "outputId": "16c352c5-0d32-4ea8-b51c-5518fdb76295"
   },
   "outputs": [],
   "source": [
    "# Display performance metrics\n",
    "print(\"mean squared error: \", mean_squared_error(y_test, y_predictions2))\n",
    "print(\"mean absolute error: \", mean_absolute_error(y_test, y_predictions2))\n",
    "print(\"R¬≤: \", r2_score(y_test, y_predictions2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bufv1jNhY-xu"
   },
   "source": [
    "## Feature Importance: What features are important for prediction?\n",
    "\n",
    "An interesting byproduct of building a decision tree is the Feature Importance. The importance of a feature is computed as the (normalized) total reduction of the criterion (e.g. MAE) brought by that feature. It is a crcuial measure for interpreting the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "1iqYY6r1ZbqU",
    "outputId": "8e696ebd-d1d3-495e-faa0-62385371ec56"
   },
   "outputs": [],
   "source": [
    "# We get the feature importance out of the model, calculate relative values and print them as bar chart\n",
    "feature_importance = model2.feature_importances_\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.sum())\n",
    "\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.barh(pos, feature_importance[sorted_idx], align=\"center\")\n",
    "plt.yticks(pos, x_train.columns[sorted_idx])\n",
    "plt.xlabel(\"Relative Importance\")\n",
    "plt.title(\"Feature Importance\")\n",
    "display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zeBXVF0VZine"
   },
   "source": [
    "### Improving performance with decision tree ensemble methods\n",
    "Despite being a powerful concept, also decision trees have their drawbacks. They tend to overfit on the training data, leading to suboptimal generalization capabilities. Generally, they don't yield a high prediction accuracy in many situations. Decision tree ensemble methods tackle these issues by building on the concept of randomization. Random Forest includes many trees in a single model (=forest) but only consides subsets of training data and input features for building the individual tree. In practice, this method achieves significantly higher robustness and performance compared to a singular decision tree. It is a popular choice for a wide range of real-world data science use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SMkjv7VgZqZ2"
   },
   "outputs": [],
   "source": [
    "# We need to create an object of the RandomForestRegressor() class. n_estimators defines the amount of trees in the forest, n_jobs at -1 activates mulitcore processing\n",
    "model3 = RandomForestRegressor(n_estimators=100, n_jobs=-1)\n",
    "model3.fit(x_train, y_train)\n",
    "y_predictions3 = model3.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rlca9hlUZ5rL",
    "outputId": "b8e8045f-dc9d-4d41-b139-2723e5b8b893"
   },
   "outputs": [],
   "source": [
    "print(\"mean squared error: \", mean_squared_error(y_test, y_predictions3))\n",
    "print(\"mean absolute error: \", mean_absolute_error(y_test, y_predictions3))\n",
    "print(\"R¬≤: \", r2_score(y_test, y_predictions3))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
